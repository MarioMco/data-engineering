## Web scraping with Scrapy
In this example we scrape product information from a webshop for items related to the search term "rtx".

<img src="./pics/webshop.png" alt="Webshop" title="Webshop">


The scraped data has been saved in PostgreSQL DB and it includes the product url, name, info, description, price and price without tax.

<img src="./pics/DB data.png" alt="DB data" title="DB data">

### Libraries Used
Scrapy: Utilized for parsing HTML content.
psycopg2: Used to store data in PostgreSQL database.
urllib.parse: Used to encode username and password
dotenv: Used to load environment variables from a .env
requests: Used to fetch the data

### Scrapy Start Steps
In terminal:
  1. `scrapy startproject webshopscraper`
  2. `scrapy genspider webshopspider <site_url>`
  3. In the scapy.cfg under the settings add: `shell = ipython` so we can use it with `scrapy shell`.

### webshopspider
`spider` is a Python class that defines how to scrape certain information from a website. It determines what pages to scrape, how to follow links, and how to extract data from the HTML structure of web pages. In our example we have:

`WebshopspiderSpider` Class:
 - name: The name of the spider.
 - allowed_domains: A list of allowed domains for the spider to crawl.
 - start_urls: A list of URLs from which the spider will start crawling.

`parse` Method:
 - This method is the default callback function for processing the initial response and subsequent responses generated by following links.
 - It extracts product URLs from the initial page and sends requests to scrape the details of each product.
 - It also looks for a "next page" link and recursively follows it.

`parse_product_page` Method:
 - This method is a callback function to parse the details of each product page.
 - It initializes a `ProductItem` object and populates its fields with data extracted from the product page using CSS selectors.
 - It yields the populated `ProductItem`.

This spider is designed to scrape product information from a webshop website and populate instances of `ProductItem` with the extracted data, following links to subsequent pages as needed.

### items

`items` are simple containers used to collect and store scraped data. They provide a structured and consistent way to organize the data that spider extracts from web pages. In our case we have created `ProductItem`.

### pipelines

`pipelines` are a mechanism for processing items (the structured data extracted by spiders) before they are stored or further processed. Pipelines are used for tasks such as cleaning, validating, and storing data. They provide a way to define reusable components for processing items as they pass through the Scrapy system.

Our contains two pipeline classes: `WebshopscraperPipeline` and `SaveToPostgreSQLPipeline`.

#### WebshopscraperPipeline:
This pipeline performs some data cleaning and transformation tasks on the items passed through it before they're further processed or stored.

- `process_item` method:
  - It takes the item and spider as parameters.
  - It uses ItemAdapter from Scrapy to simplify item handling.
  - Cleaning whitespace: It strips leading and trailing whitespaces from certain fields (product, description, product_info, product_url).
  - Converting price to float: It converts the price and price_excl_tax fields to float.
  - Return value: It returns the processed item after modifications.

#### SaveToPostgreSQLPipeline:
This pipeline is responsible for saving the items into a PostgreSQL database.

`__init__` method:
  - It initializes the pipeline by establishing a connection to the PostgreSQL database.
  - It creates a table named products if it doesn't already exist, defining its schema.
`process_item` method:
  - It takes the item and spider as parameters.
  - It inserts the item's data into the products table in the PostgreSQL database.
  - It commits the transaction to ensure the changes are saved.
`close_spider` method:
  - It's called when the spider is closed.
  - It closes the cursor and connection to the PostgreSQL database.

#### Enabling Pipelines:
Pipelines need to be enabled in the Scrapy settings before they can be used. You can enable and configure pipelines by adding them to the ITEM_PIPELINES setting in your Scrapy project's settings file. In our example we have:
`ITEM_PIPELINES = {`
`"webshopscraper.pipelines.WebshopscraperPipeline": 300,`
`"webshopscraper.pipelines.SaveToPostgreSQLPipeline": 400,}`
- The numbers `300` and `400` in the `ITEM_PIPELINES` setting represent the order in which pipelines are executed. Lower numbers indicate higher priority, meaning pipelines with lower numbers will be executed before pipelines with higher numbers. So, in this case, WebshopscraperPipeline will process items first, followed by SaveToPostgreSQLPipeline.


### middlewares
Middleware is a framework of hooks into Scrapy's request/response processing. It's a lightweight, low-level plugin system that allows you to manipulate requests and responses at different stages of the Scrapy spider's lifecycle.

Middleware sits between the engine (which manages the flow of requests and responses) and the downloader (which fetches web pages). It intercepts requests before they're sent to the downloader and responses before they're returned to the spider.

Middleware is useful for a variety of tasks, including:

 - User-Agent Rotation: Changing the user-agent for each request to prevent detection or to mimic different browsers.
 - Proxy Rotation: Switching between different proxies to avoid IP bans or to route requests through different IP addresses.
 - Request/Response Modification: Manipulating request headers, cookies, URLs, or response content.
 - Error Handling: Handling exceptions, retries, or other errors that occur during the crawling process.
 - Throttling and Rate Limiting: Controlling the rate at which requests are sent to avoid overloading servers.
 - Logging: Logging information about requests, responses, or errors for debugging purposes.

Scrapy provides a set of built-in middleware, and we write our own custom middleware to extend or modify Scrapy's behavior to suit our specific needs.

Middleware components are configured in the `DOWNLOADER_MIDDLEWARES` setting in a Scrapy project's settings file. Each middleware component can define methods that handle different stages of the request/response processing lifecycle. In our example we have:
`DOWNLOADER_MIDDLEWARES = {`
   `"webshopscraper.middlewares.ScrapeOpsFakeBrowserHeaderAgentMiddleware": 400,`
   `"webshopscraper.middlewares.MyProxyMiddleware": 350,}`

#### ScrapeOpsFakeBrowserHeaderAgentMiddleware:
This middleware class is responsible for injecting random browser headers into the HTTP request headers. It uses the ScrapeOps API to fetch a list of fake browser headers and randomly selects one to add to each request.

#### MyProxyMiddleware:
This middleware class is responsible for handling proxy requests. It sets up HTTP basic authentication with the provided username and password, then attaches this authentication information to the request's headers.

